{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@dataset{banda_juan_m_2020_3757272,\n",
    "  author       = {Banda, Juan M. and\n",
    "                  Tekumalla, Ramya and\n",
    "                  Wang, Guanyu and\n",
    "                  Yu, Jingyuan and\n",
    "                  Liu, Tuo and\n",
    "                  Ding, Yuning and\n",
    "                  Artemova, Katya and\n",
    "                  Tutubalin–∞, Elena and\n",
    "                  Chowell, Gerardo},\n",
    "  title        = {{A large-scale COVID-19 Twitter chatter dataset for \n",
    "                   open scientific research - an international\n",
    "                   collaboration}},\n",
    "  month        = may,\n",
    "  year         = 2020,\n",
    "  note         = {{This dataset will be updated bi-weekly at least \n",
    "                   with additional tweets, look at the github repo\n",
    "                   for these updates. Release: We have standardized\n",
    "                   the name of the resource to match our pre-print\n",
    "                   manuscript and to not have to update it every\n",
    "                   week.}},\n",
    "  publisher    = {Zenodo},\n",
    "  version      = {54.0},\n",
    "  doi          = {10.5281/zenodo.3723939},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3723939}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following extracting data function is follow from https://github.com/thepanacealab/covid19_twitter/blob/master/COVID_19_dataset_Tutorial.ipynb\n",
    "from IPython.display import clear_output\n",
    "# #Twarc\n",
    "# !pip install twarc\n",
    "# # Tweepy 3.8.0\n",
    "# !pip install tweepy\n",
    "# #Argparse 3.2\n",
    "# !pip install argparse\n",
    "# #Xtract 0.1 a3\n",
    "# !pip install xtract\n",
    "# #Wget 3.2\n",
    "# !pip install wget\n",
    "# clear_output()\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import wget\n",
    "import csv\n",
    "import linecache\n",
    "from shutil import copyfile\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid flooding gitlab, the data folder is not uploaded, so we need to created it first\n",
    "datafolder = \".\\\\data\\\\\"\n",
    "try:\n",
    "    os.mkdir(datafolder)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % datafolder)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % datafolder)\n",
    "\n",
    "from datetime import timedelta, date\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "#Extract all the daily tweets from 2020-03-22 to 2021-03-22\n",
    "#The git respositry is https://github.com/GrimmNoters/covid19_twitter\n",
    "#This is a forked version from the origin database\n",
    "#Unfortunately the database only provide tweets that specified language from 2020-07-26 until now, so we skipped the first four months.\n",
    "start_date = date(2020, 8, 1)\n",
    "end_date = date(2021, 3, 23)\n",
    "d= None\n",
    "dataset_URL= None\n",
    "readfileN= None\n",
    "savefileN = None\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    d = single_date.strftime(\"%Y-%m-%d\")\n",
    "    #Clean would only retrieve the origin tweets but ignore retweets\n",
    "    dataset_URL = \"https://github.com/GrimmNoters/covid19_twitter/blob/master/dailies/\",d,\"/\",d,\"_clean-dataset.tsv.gz?raw=true\"\n",
    "    dataset_URL=''.join(dataset_URL)\n",
    "    #Downloads the dataset (compressed in a GZ format)\n",
    "    #!wget dataset_URL -O date.tsv.gz\n",
    "    readfileN = d,\".tsv.gz\"\n",
    "    readfileN=''.join(readfileN)\n",
    "    savefileN = \".\\\\data\\\\\",d,\".tsv\"\n",
    "    savefileN=''.join(savefileN)\n",
    "    print(dataset_URL)\n",
    "#     Examine if local has saved this date's data\n",
    "    if(os.path.exists(savefileN)):\n",
    "        continue\n",
    "    wget.download(dataset_URL, out=readfileN)\n",
    "    #Unzips the dataset and gets the TSV dataset\n",
    "    with gzip.open(readfileN, 'rb') as f_in:\n",
    "        with open(savefileN, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    #Deletes the compressed GZ file\n",
    "    os.unlink(readfileN)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For our project we only use English\n",
    "#Also due to api limit we only select random 100 users for each day.\n",
    "import random\n",
    "filtered_language = \"en\"\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    date = single_date.strftime(\"%Y-%m-%d\")\n",
    "    readfileN = \".\\\\data\\\\\",date,\".tsv\"\n",
    "    readfileN=''.join(readfileN)\n",
    "    savefileN = \".\\\\data\\\\\",date,\"-filtered.tsv\"\n",
    "    savefileN=''.join(savefileN)\n",
    "    filtered_tw = list()\n",
    "    current_line = 1\n",
    "    tmp = list()\n",
    "    with open(readfileN) as tsvfile:\n",
    "        if(os.path.exists(savefileN)):\n",
    "            continue\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        \n",
    "        if current_line == 1:\n",
    "            tmp.append(linecache.getline(readfileN, current_line))\n",
    "            \n",
    "            for line in tsvreader:\n",
    "                if line[3] == filtered_language:\n",
    "                    filtered_tw.append(linecache.getline(readfileN, current_line))\n",
    "                current_line += 1\n",
    "        for key in random.sample(filtered_tw, 100):\n",
    "            tmp.append(key)\n",
    "        filtered_tw = tmp\n",
    "    print(readfileN,'Filtered')\n",
    "    with open(savefileN, 'w') as f_output:\n",
    "        for item in filtered_tw:\n",
    "            f_output.write(item)\n",
    "#     os.remove(readfileN)\n",
    "clear_output() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import tweepy\n",
    "# from tweepy import OAuthHandler\n",
    "# #Create a JSON file unless existed\n",
    "# if(not os.path.exists(\"api_keys.json\")):\n",
    "#     # Authenticate\n",
    "#     CONSUMER_KEY = \"\"\n",
    "#     CONSUMER_SECRET_KEY = \"\"\n",
    "#     ACCESS_TOKEN_KEY = \"\"\n",
    "#     ACCESS_TOKEN_SECRET_KEY = \"\"\n",
    "\n",
    "#     #Creates a JSON Files with the API credentials\n",
    "#     with open('api_keys.json', 'w') as outfile:\n",
    "#         json.dump({\n",
    "#         \"consumer_key\":CONSUMER_KEY,\n",
    "#         \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
    "#         \"access_token\":ACCESS_TOKEN_KEY,\n",
    "#         \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY\n",
    "#      }, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The lines below are just to test if the twitter credentials are correct\n",
    "# # Authenticate\n",
    "# auth = tweepy.AppAuthHandler(CONSUMER_KEY, CONSUMER_SECRET_KEY)\n",
    "\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# if (not api):\n",
    "#    print (\"Can't Authenticate\")\n",
    "#    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_acquisition/get_metadata.py -O get_metadata.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     args = easydict.EasyDict({\n",
    "#         \"inputfile\": \"\",\n",
    "#         \"outputfile\": \"\",\n",
    "#         \"keyfile\": \"\",\n",
    "#         \"idcolumn\": None,\n",
    "#         \"mode\": None,\n",
    "#     })\n",
    "from imp import reload\n",
    "import get_metadata\n",
    "reload(get_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydrating the data\n",
    "folder = \"data\"\n",
    "keyfile = \"api_keys.json\"\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    date = single_date.strftime(\"%Y-%m-%d\")\n",
    "    infile = \".\\\\data\\\\\",date,\"-filtered.tsv\"\n",
    "    infile=''.join(infile)\n",
    "    outfile = date,\"-hydrated\"\n",
    "    outfile=''.join(outfile)\n",
    "    get_metadata.hydrating(infile,folder, outfile, keyfile, None, None)\n",
    "    clear_output() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temp files\n",
    "# Unfortunately i should not remove the json file since it can be used later when combining daily to monthly\n",
    "# but so far I have reach the limit of Twitter api request so I could not retrieve the files again and recreate the json files.\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    date = single_date.strftime(\"%Y-%m-%d\")\n",
    "    file = date,\"-hydrated\"\n",
    "    file=''.join(file)\n",
    "    if(os.path.exists(file)):\n",
    "        os.remove(file)\n",
    "    file = file,\"_short.json\"\n",
    "    file=''.join(file)\n",
    "    if(os.path.exists(file)):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted dates\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "def end_of_month(dt):\n",
    "    todays_month = dt.month\n",
    "    tomorrows_month = (dt + datetime.timedelta(days=1)).month\n",
    "    return True if tomorrows_month != todays_month else False\n",
    "def start_of_month(dt):\n",
    "    return True if dt.day == 1 else False\n",
    "start_date = date(2020, 8, 1)\n",
    "end_date = date(2021, 3, 23)\n",
    "file_Date = [[]]*8\n",
    "i = 0\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    if(start_of_month(single_date)):\n",
    "        file_Date[i] = [single_date.strftime(\"%Y-%m-%d\")]\n",
    "    else:\n",
    "        file_Date[i].append(single_date.strftime(\"%Y-%m-%d\"))\n",
    "#     file_Date[i].append([single_date.strftime(\"%Y-%m-%d\")])\n",
    "    if(end_of_month(single_date)):\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data of the same month\n",
    "outF = \"\"\n",
    "import pandas as pd\n",
    "for i in range(len(file_Date)):\n",
    "    outF = ''.join((\".\\\\data\\\\\",file_Date[i][0][:7]+\"_hydrated.csv\"))\n",
    "    combined_csv = pd.concat([pd.read_csv(''.join((\".\\\\data\\\\\",one_day,\"-hydrated.csv\"))) for one_day in file_Date[i]])\n",
    "    combined_csv.to_csv(outF, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
