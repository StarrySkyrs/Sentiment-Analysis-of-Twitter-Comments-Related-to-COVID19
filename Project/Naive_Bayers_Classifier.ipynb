{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv6zDpEKiq31"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Clean Tweet Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ytUW6NBoHAi"
   },
   "outputs": [],
   "source": [
    "# Data clean\n",
    "def data_clean(data:list):\n",
    "  text = []\n",
    "  for t in data:\n",
    "    temp = t.lstrip('b\\'')\n",
    "    temp = temp.rstrip('\\'')\n",
    "    cle = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', temp)\n",
    "    cle = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", cle)\n",
    "    cle = re.sub(r'(\\\\x(.){2})', '', cle)\n",
    "    cle = re.sub('[0-9]', '', cle)\n",
    "    cle = re.sub(r'(\\\\n)', ' ', cle)\n",
    "    cle = re.sub(r'[^\\w\\s]','', cle)\n",
    "    cle = cle.lower()\n",
    "    text.append(cle)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_lemmatized(tweet_tokens, stopword_list:list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweets_lemmatized = []\n",
    "    for word, tag in pos_tag(tweet_tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        _token = lemmatizer.lemmatize(word, pos)\n",
    "        if _token.lower() not in stopword_list:\n",
    "            tweets_lemmatized.append(_token.lower())\n",
    "#             for item in nltk.bigrams (_token.split()):\n",
    "#                 tweets_lemmatized.append(' '.join(item))\n",
    "#             for item in nltk.trigrams (_token.split()):\n",
    "#                 tweets_lemmatized.append(' '.join(item))\n",
    "    return tweets_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = \"I am trying to workd on a bigram!\"\n",
    "# tweets_lemmatized(data_clean([h]),stopword_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier using nltk twitter sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# Import and stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopword_List = stopwords.words('english')\n",
    "#nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSingleWordToTrainDict(m_word:str,m_argument:str,m_stopword_list:list):\n",
    "    m_t = tweets_lemmatized([m_word],m_stopword_list)\n",
    "    return ({m_t[i]:True for i in range(len(m_t))},m_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some positive & negative words\n",
    "# The dictionary is retireved from https://github.com/leelaylay/TweetSemEval/tree/master/dataset/dict\n",
    "posFile = '.\\\\dict\\\\positive-words.txt'\n",
    "negFile = '.\\\\dict\\\\negative-words.txt'\n",
    "sentimentDict = []\n",
    "with open(posFile,'r') as posFileReader:\n",
    "    for single_word in posFileReader:\n",
    "        sentimentDict.append(makeSingleWordToTrainDict(single_word.rstrip(\"\\n\"),'Positive',stopword_List))\n",
    "with open(negFile,'r') as negFileReader:\n",
    "    for single_word in negFileReader:\n",
    "        sentimentDict.append(makeSingleWordToTrainDict(single_word.rstrip(\"\\n\"),'Negative',stopword_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "from nltk.corpus import twitter_samples\n",
    "sample_pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "sample_neg_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordListToTrainDict(mwl_word:list,mwl_argument:str,mwl_stopword_list:list):\n",
    "    mwl_t = tweets_lemmatized(mwl_word,mwl_stopword_list)\n",
    "    return ({mwl_t[i]:True for i in range(len(mwl_t))},mwl_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkTweetData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# Clean sample and Tokennized, then generate to dictonary list\n",
    "sample_pos_tweets_clean = data_clean(sample_pos_tweets)\n",
    "sample_neg_tweets_clean = data_clean(sample_neg_tweets)\n",
    "for _pos_tweet in sample_pos_tweets_clean:\n",
    "    _pos_tokenized = TweetTokenizer().tokenize(_pos_tweet)\n",
    "    _pos_dict = makeWordListToTrainDict(_pos_tokenized,'Positive',stopword_List)\n",
    "    nltkTweetData.append(_pos_dict)\n",
    "for _neg_tweet in sample_neg_tweets_clean:\n",
    "    _neg_tokenized = TweetTokenizer().tokenize(_neg_tweet)\n",
    "    _neg_dict = makeWordListToTrainDict(_neg_tokenized,'Negative',stopword_List)\n",
    "    nltkTweetData.append(_neg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(nltkTweetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sentimentDict + nltkTweetData[:7000]\n",
    "test_data = nltkTweetData[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the classifier to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(text: str):\n",
    "  cleaned_tweet = data_clean([text])\n",
    "  tmp_list = []\n",
    "  for x in cleaned_tweet:\n",
    "    tmp_list.append(TweetTokenizer().tokenize(x))\n",
    "  result = classifier.classify(dict([token, True] for token in tmp_list[0]))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a month iterator\n",
    "from datetime import timedelta, date\n",
    "def monthrange(m_start_date, m_end_date):\n",
    "    for n in range(int((m_end_date - m_start_date).days/31)+1):\n",
    "        yield m_start_date + timedelta(n*31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have multiple test data, the classfiy process is packaged as a function\n",
    "def classifyTestData(testNum:int):\n",
    "    _start_date = date(2020, 8, 1)\n",
    "    _end_date = date(2021, 3, 23)\n",
    "    classifyRes = []\n",
    "    for single_month in monthrange(_start_date, _end_date):\n",
    "        month = single_month.strftime(\"%Y-%m\")\n",
    "        fileFolder = \".\\\\test_data_\",str(testNum),\"\\\\\"\n",
    "        fileFolder=''.join(fileFolder)\n",
    "        monthCSV = fileFolder,month,\"_hydrated.csv\"\n",
    "        monthCSV=''.join(monthCSV)\n",
    "        data = pd.read_csv(monthCSV)\n",
    "        negCnt = 0\n",
    "        posCnt = 0\n",
    "        for text in data[\"text\"]:\n",
    "            if(result(text)== \"Positive\"):\n",
    "                posCnt = posCnt +1\n",
    "            else:\n",
    "                negCnt = negCnt +1\n",
    "        classifyRes.append([posCnt,negCnt,len(data[\"text\"])])\n",
    "    return classifyRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderNum = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allClassified = []\n",
    "for i in range(folderNum):\n",
    "    allClassified.append(classifyTestData(i+1))\n",
    "allClassified = np.array(allClassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = len(allClassified[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better View, we round the result to 4 decimals\n",
    "pnRate = []\n",
    "avgPNRate = []\n",
    "for j in range(month):\n",
    "    posRate = 0\n",
    "    negRate = 0\n",
    "    for i in range(folderNum):\n",
    "        currentPosRate = allClassified[i][j][0]/allClassified[i][j][2]\n",
    "        currentNegRate = allClassified[i][j][1]/allClassified[i][j][2]\n",
    "        posRate = posRate + currentPosRate\n",
    "        negRate = negRate + currentNegRate\n",
    "        pnRate.append([np.round(currentPosRate,4),np.round(currentNegRate,4)])\n",
    "    avgPNRate.append([np.round(posRate/folderNum,4),np.round(negRate/folderNum,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2020, 8, 1)\n",
    "end_date = date(2021, 3, 23)\n",
    "months = []\n",
    "for single_month in monthrange(start_date, end_date):\n",
    "    months.append(single_month.strftime(\"%Y-%m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPNRateDT = pd.DataFrame(data = avgPNRate, columns = ['Positive','Negative'],index = months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPNRateDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPNRateDT.plot.bar(stacked=True, alpha=0.5,) \n",
    "plt.title(\"Average Positive and Negative Rate towards Covid-19\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnRate = np.array(pnRate)\n",
    "for f in range(folderNum):\n",
    "    subPNRateDT = []\n",
    "    for i in range(month):\n",
    "        subPNRateDT.append(pnRate[f*month+i])\n",
    "    subPNRateDT = np.array(subPNRateDT)\n",
    "    subPNRateDT = pd.DataFrame(data = avgPNRate, columns = ['Positive','Negative'],index = months)\n",
    "    print(subPNRateDT)\n",
    "    subPNRateDT.plot.bar(stacked=True, alpha=0.5,) \n",
    "    title = \"Folder test_data_\",str(f+1), \" Positive and Negative Rate towards Covid-19\"\n",
    "    title=''.join(title)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled18.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
