{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean\n",
    "def data_clean(data:list):\n",
    "  text = []\n",
    "  for t in data:\n",
    "    temp = t.lstrip('b\\'')\n",
    "    temp = temp.rstrip('\\'')\n",
    "    cle = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', temp)\n",
    "    cle = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", cle)\n",
    "    cle = re.sub(r'(\\\\x(.){2})', '', cle)\n",
    "    cle = re.sub('[0-9]', '', cle)\n",
    "    cle = re.sub(r'(\\\\n)', ' ', cle)\n",
    "    cle = re.sub(r'[^\\w\\s]','', cle)\n",
    "    cle = cle.lower()\n",
    "    text.append(cle)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_lemmatized(tweet_tokens, stopword_list:list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweets_lemmatized = []\n",
    "    for word, tag in pos_tag(tweet_tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        _token = lemmatizer.lemmatize(word, pos)\n",
    "        if _token.lower() not in stopword_list:\n",
    "            tweets_lemmatized.append(_token.lower())\n",
    "    return tweets_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# Import and stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopword_List = stopwords.words('english')\n",
    "#nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderNum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a month iterator\n",
    "from datetime import timedelta, date\n",
    "def monthrange(m_start_date, m_end_date):\n",
    "    for n in range(int((m_end_date - m_start_date).days/31)+1):\n",
    "        yield m_start_date + timedelta(n*31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "start_date = date(2020, 8, 1)\n",
    "end_date = date(2021, 3, 23)\n",
    "monthrangeData = []\n",
    "for single_month in monthrange(start_date, end_date):\n",
    "    tmp = []\n",
    "    for i in range(folderNum):\n",
    "        month = single_month.strftime(\"%Y-%m\")\n",
    "        fileFolder = \".\\\\test_data_\",str(i+1),\"\\\\\"\n",
    "        fileFolder=''.join(fileFolder)\n",
    "        monthCSV = fileFolder,month,\"_hydrated.csv\"\n",
    "        monthCSV=''.join(monthCSV)\n",
    "        data = pd.read_csv(monthCSV)\n",
    "        text = data_clean(data[\"text\"])\n",
    "        text = tweets_lemmatized(text,stopword_List)\n",
    "        for t in text:\n",
    "            tmp.append(' '.join(t.split()))\n",
    "    monthrangeData.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')\n",
    "included_tags = {\"VERB\", \"PROPN\",\"NOUN\"}\n",
    "# Some how the spacy marked auxiliary verbs as verbs, so we remove it \n",
    "# Also have is used in both auxiliary verb and verb, to reduce noise we remove it.\n",
    "auxiliary_verb = {'have','be','is','are','am','was','were','being','been'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPoping(monthData):\n",
    "    for d in monthData:\n",
    "        yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_monthrange = []\n",
    "md = dataPoping(monthrangeData)\n",
    "for single_month in monthrange(start_date, end_date):\n",
    "    print(\"Working on\",single_month.strftime(\"%Y-%m\"))\n",
    "    tmp = md.__next__()\n",
    "    clean_tmp = []\n",
    "    for t in tmp:\n",
    "        for token in sp(t):\n",
    "            if token.pos_ in included_tags:\n",
    "                if token.text not in auxiliary_verb:\n",
    "                    clean_tmp.append(token.text)\n",
    "    clean_monthrange.append(clean_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datafolder = \".\\\\analysis\\\\\"\n",
    "try:\n",
    "    os.mkdir(datafolder)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % datafolder)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "cmd = dataPoping(clean_monthrange)\n",
    "for single_month in monthrange(start_date, end_date):\n",
    "    this_month = single_month.strftime(\"%Y-%m\")\n",
    "    print(\"Working on\",this_month)\n",
    "    clean_tmp = cmd.__next__()\n",
    "    req_dist = FreqDist(clean_tmp)\n",
    "    save_freq = '.\\\\analysis\\\\',this_month,\"-frequence.txt\"\n",
    "    save_freq=''.join(save_freq)\n",
    "    with open(save_freq, 'w') as f_out:\n",
    "        for t in req_dist.most_common(100):\n",
    "            word,freq = t[0],t[1]\n",
    "            f_out.write(word)\n",
    "            f_out.write(' ')\n",
    "            f_out.write(str(freq))\n",
    "            f_out.write('\\n')\n",
    "    str1 = ' '.join(str(e) for e in clean_tmp)\n",
    "    wordcloud = WordCloud(max_words=100, background_color=\"white\",width=8000, height=4000).generate(str1)\n",
    "    save_img = '.\\\\analysis\\\\',this_month,\".png\"\n",
    "    save_img=''.join(save_img)\n",
    "    wordcloud.to_file(save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
